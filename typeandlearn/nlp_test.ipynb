{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "d6dc92e2",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[nltk_data] Downloading package punkt_tab to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package punkt_tab is already up-to-date!\n",
                  "[nltk_data] Downloading package wordnet to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package wordnet is already up-to-date!\n",
                  "[nltk_data] Downloading package omw-1.4 to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
                  "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
                  "[nltk_data]       date!\n"
               ]
            }
         ],
         "source": [
            "import nltk\n",
            "nltk.download('punkt_tab')      \n",
            "nltk.download('wordnet')    \n",
            "nltk.download('omw-1.4') \n",
            "nltk.download('averaged_perceptron_tagger_eng') \n",
            "\n",
            "file_path = \"../test_files/test_text.txt\"\n",
            "\n",
            "with open(file_path, \"r\") as file:\n",
            "    test_text = file.read()\n",
            "\n",
            "# https://www.geeksforgeeks.org/python/python-lemmatization-with-nltk/\n",
            "\n",
            "from nltk.tokenize import word_tokenize\n",
            "from nltk import pos_tag\n",
            "from nltk.stem import WordNetLemmatizer\n",
            "\n",
            "from nltk.probability import FreqDist\n",
            "\n",
            "import pandas as pd"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "5cd4d757",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[nltk_data] Downloading package punkt to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package punkt is already up-to-date!\n",
                  "[nltk_data] Downloading package punkt_tab to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package punkt_tab is already up-to-date!\n",
                  "[nltk_data] Downloading package wordnet to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package wordnet is already up-to-date!\n",
                  "[nltk_data] Downloading package omw-1.4 to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
                  "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
                  "[nltk_data]     /Users/irajnelson/nltk_data...\n",
                  "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
                  "[nltk_data]       date!\n"
               ]
            },
            {
               "ename": "NameError",
               "evalue": "name 'word_tokenize' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m test_str = test_text\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtext_processing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lemmatized_frequency_distribution, clean_freq_dist\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m lem_freq = \u001b[43mlemmatized_frequency_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m clean_freq = clean_freq_dist(test_str)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLemmatized Frequency Distribution:\u001b[39m\u001b[33m\"\u001b[39m, lem_freq)\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/typeandlearn/typeandlearn/text_processing.py:29\u001b[39m, in \u001b[36mlemmatized_frequency_distribution\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatized_frequency_distribution\u001b[39m(text):\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    Get the lemmatized frequency distribution of a text\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03m    freq_dist: DataFrame, lemmatized frequency distribution\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m(text.lower())\n\u001b[32m     30\u001b[39m     lemmatizer = WordNetLemmatizer()\n\u001b[32m     31\u001b[39m     tagged_tokens = pos_tag(tokens)\n",
                  "\u001b[31mNameError\u001b[39m: name 'word_tokenize' is not defined"
               ]
            }
         ],
         "source": [
            "\n",
            "\n",
            "# Test for all lemmatized_frequency_distribution and clean_freq_dist from text_processing.py\n",
            "test_str = test_text\n",
            "from text_processing import lemmatized_frequency_distribution, clean_freq_dist\n",
            "\n",
            "lem_freq = lemmatized_frequency_distribution(test_str)\n",
            "clean_freq = clean_freq_dist(test_str)\n",
            "\n",
            "print(\"Lemmatized Frequency Distribution:\", lem_freq)\n",
            "print(\"Clean Frequency Distribution:\", clean_freq)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "14fb6e74",
         "metadata": {},
         "outputs": [],
         "source": [
            "test_str = \"The running boy ran his daily run because he runs fast.\"\n",
            "test_str = test_text\n",
            "\n",
            "# Tokenize the string by word (break it into the individual words)\n",
            "tokenized_str = word_tokenize(test_str)\n",
            "tokenized_str\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "87e95f31",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Lemmatize the string, reduce words to base form\n",
            "\n",
            "lemmatizer = WordNetLemmatizer()\n",
            "tagged_tokenized_str = pos_tag(tokenized_str)\n",
            "\n",
            "# todo: use:\n",
            "# wordnet.NOUN = 'n'\n",
            "# wordnet.VERB = 'v'\n",
            "# wordnet.ADJ  = 'a'\n",
            "# wordnet.ADV  = 'r'\n",
            "def get_wordnet_pos(tag):\n",
            "    if tag.startswith('J'):\n",
            "        return 'a'\n",
            "    elif tag.startswith('V'):\n",
            "        return 'v'\n",
            "    elif tag.startswith('N'):\n",
            "        return 'n'\n",
            "    elif tag.startswith('R'):\n",
            "        return 'r'\n",
            "    else:\n",
            "        return 'n'\n",
            "\n",
            "\n",
            "lemmatized_str = []\n",
            "for word, tag in tagged_tokenized_str:\n",
            "    if word.lower() == 'are' or word.lower() in ['is', 'am']:\n",
            "        lemmatized_str.append(word)\n",
            "    else:\n",
            "        lemmatized_str.append(\n",
            "            lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
            "\t\t\t\n",
            "lemmatized_str\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2f862198",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Make frequency distribution\n",
            "\n",
            "fdist = FreqDist()\n",
            "for word in lemmatized_str:\n",
            "\tfdist[word.lower()] += 1\n",
            "\n",
            "fdist\n",
            "\n",
            "## Write fdist to csv file\n",
            "import csv\n",
            "with open('./test_files/fdist.csv', 'w', newline='') as csvfile:\n",
            "\t\twriter = csv.writer(csvfile)\n",
            "\t\twriter.writerow(['Word', 'Frequency'])\n",
            "\t\tfor word, frequency in fdist.items():\n",
            "\t\t\t\twriter.writerow([word, frequency])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "316240fc",
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "\n",
            "df = pd.read_csv(\"test_files/fdist.csv\")\n",
            "\n",
            "# Drop rows with non letter values\n",
            "df = df[df['Word'].str.contains(r'[a-zA-Z]')]\n",
            "\n",
            "# Drop rows with to be verbs\n",
            "to_be_verbs = ['am', 'is', 'are', 'was', 'were', 'be', 'being', 'been']\n",
            "df = df[~df['Word'].isin(to_be_verbs)]\n",
            "\n",
            "# Drop rows with prepositions without using regex\n",
            "prepositions = ['a', 'an', 'the', 'to', 'of', 'and', 'or', 'in', 'on', 'at', 'by', 'for', 'with', 'from', 'into', 'out', 'over', 'up', 'down', 'off', 'about', 'after', 'against', 'along', 'among', 'around', 'as', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'but', 'by', 'down', 'during', 'except', 'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'next', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'round', 'save', 'since', 'than', 'through', 'throughout', 'till', 'times', 'to', 'toward', 'towards', 'under', 'until', 'up', 'upon', 'via', 'with', 'within', 'without', 'yet']\n",
            "df = df[~df['Word'].isin(prepositions)]\n",
            "\n",
            "# Drop rows with conjunctions\n",
            "conjunctions = ['and', 'or', 'but', 'nor', 'for', 'yet', 'so', 'if', 'while', 'as', 'because', 'since', 'unless', 'once', 'when', 'till', 'unless', 'although', 'whereas', 'whether', 'how', 'although', 'whereas', 'whether', 'how', 'once', 'after', 'before', 'over', 'under', 'above', 'below', 'among', 'amid', 'amongst', 'among', 'around', 'as', 'at', 'atop', 'by', 'down', 'during', 'except', 'for', 'from', 'in', 'into', 'like', 'minus', 'near', 'next', 'of', 'off', 'on', 'onto', 'opposite', 'out', 'outside', 'over', 'past', 'per', 'plus', 'round', 'save', 'since', 'than', 'through', 'throughout', 'till', 'times', 'to', 'toward', 'towards', 'under', 'until', 'up', 'upon', 'via', 'with', 'within', 'without', 'yet']\n",
            "df = df[~df['Word'].isin(conjunctions)]\n",
            "\n",
            "# Drop rows with articles\n",
            "articles = ['a', 'an', 'the']\n",
            "df = df[~df['Word'].isin(articles)]\n",
            "\n",
            "# Drop rows with pronouns\n",
            "pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'my', 'him', 'her', 'us', 'mine', 'our', 'his', 'hers', 'its', 'ours', 'yours', 'theirs', 'myself', 'yourself', 'himself', 'herself', 'itself', 'yourselves', 'themselves', 'my', 'your', 'yours', 'ours', 'theirs', 'its', 'whose']\n",
            "df = df[~df['Word'].isin(pronouns)]\n",
            "\n",
            "# Sort by frequency\n",
            "df = df.sort_values(by=['Frequency'], ascending=False)\n",
            "\n",
            "df.head(10) "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e392ace1",
         "metadata": {},
         "outputs": [],
         "source": [
            "# write first 50 rows from df to a md file\n",
            "\n",
            "df_50 = df.head(50)\n",
            "\n",
            "# Convert to Markdown string\n",
            "markdown_table = df_50.to_markdown(index=False)\n",
            "# Write to a text file\n",
            "with open('./test_files/freq_table_prompt.md', 'w') as file:\n",
            "    file.write(\"Here is the word frequency table:\\n\\n\")\n",
            "    file.write(markdown_table)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "00ddee05",
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.14.2"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
